---
title: "efa_cfa"
output:
  pdf_document: default
  html_notebook: default
  html_document: default
---
```{r echo = FALSE, messages = FALSE}
library(foreign)
library(corrplot)
library(car)
library(ltm)
library(lavaan)
library(psych)
library(semTools)
library(semPlot)
library(magrittr)
library(tidyr)
library(dplyr)
library(ggplot2)
library(rgdal)
library(sp)
library(MatchIt)

```
Note: On 9/20 I removed all the extra code that is pulling variables from mor2 that aren't being used in the EFA process.  
All we really need here are the ecological parameters and the Ref Number IDs so we can link it back with the other data on practices, etc. later.

### Factor Analysis:
The main applications of factor analytic techniques are: (1) to reduce the number of variables and (2) to detect structure in the relationships between variables, that is to classify variables. Therefore, factor analysis is applied as a data reduction or structure detection method 




#### Info on the DATA: 

In order to execute this file you need to have already run the following:
1. hh_trends.R
2. soil_params.R

### Data screening:

```{r echo=FALSE}
mor2<- foreign::read.spss("./data/ALL_EcolHHOrgMerged_byUvuljaa_03_15_17_Simplified.sav" , 
                 to.data.frame=TRUE,
                 use.value.labels=FALSE) #
# tidy format
tbl_df(mor2)


eco.params <- dplyr :: select(mor2, 
            # identifiers for linking w other datasets:
             RefNum = SocialSurveyReferenceNumber, 
             PlotID = UniquePlotID500,  #this one incorps the Uvuulja and site
             Lat = Latitude500, # bc need to match later at 500
             Long= Longitude500,
            # grasses:
             PerrGrass = PerGrassCover_percent_Mean500_1000,
             Grass_bm = Grass_gm2_Mean500_1000,
            # AnuGrass = AnGrassCover_percent_Mean500_1000,  #  not enough of this to use it. Mostly zeros....
             TotGrassCover = TotalGrassCover_percent_Mean500_1000,
            # Acnath = Acnath_gm2_Mean500_1000,  # almost allll zeros
            # forbs:
             PerrForb = PerForbCover_percent_Mean500_1000,
             AnuForb = AnForbCover_percent_Mean500_1000,
             Forb_bm = Forb_gm2_Mean500_1000,
            # sedges:
             Sedges = TotalSedgeCover_percent_Mean500_1000,
            # shrubs:
             Shrubs = TotalShrubCover_percent_Mean500_1000,
             TotFoliar = TotalFoliarCover_percent_Mean500_1000,
            # BasalCover = BasalCover_percent_Mean500_1000,   # something is wrong w this variable. values are huge and tiny.
            # abiotic params:
             BareSoil = BareSoil_percent_Mean500_1000,
             LitterCover = LitterCover_percent_Mean500_1000,
             Protein = CrudeProtein_percent_Mean500_1000,
            # species richness:
             SpRich = SpeciesRichnessMean500_1000
             )


#NO. BC we end up useing these ppt and ndvi vars later when matching, so we cannot also use them to decribe this as well.

# ndvi values exported from GEE:
# replace these w Jay's values when we get them:
# pttrends <- read.csv("./data/pttrends.csv", stringsAsFactors = FALSE)
# #pttrends <- readOGR(dsn = "./data", layer = "pttrends.csv")
# pttrends<- pttrends[1:130, 1:5]
# 
# geo<- pttrends$.geo
# 
# var<- sapply(pttrends$.geo, function(x) strsplit(x, split="[", fixed = TRUE))
# coord<- sapply(var, function(x) x[2])
# names(coord)<- NULL
# 
# coord<- sapply(coord, function(x) substr(x, 1, nchar(x)-2))
# split<- sapply(coord, function(x) strsplit(x, split = ","))
# pttrends$long <- sapply(split, function(x) x[1])
# pttrends$lat <- sapply(split, function(x) x[2])
# 
# preciptrend <- dplyr::select(pttrends, -.geo)
# preciptrend %<>% mutate_at(5:6, funs(as.numeric(.))) %>%
#   rename(long2 = long, lat2 = lat)

# WHY ARE THERE ALL THESE NEGATIVE VALUES???????????
# DO NOT USE MEAN OR SLOPE.


# add in the soils parameters

#library(readxl)
soils <- read_excel("/nfs/gallington-data/Herder_sem/data/Soil surface data_Ginger.xlsx", 
                    sheet = "Sheet1")
#View(soils)
soils %<>% mutate_at(9:18, funs(factor(.)))

# do these need to be flipped? so best condition is highest?
erosion.levels<- c("high erosion", "medium erosion", "little erosion")
retention.levels <- c("isolated", "fragmented", "connected")
soils$`SRD_3classes_with name` <- ordered(soils$`SRD_3classes_with name`, levels = erosion.levels)
soils$`RRC_3classes_with name` <- ordered(soils$`RRC_3classes_with name`, levels = retention.levels)
# can't average bc they are categorical so chose to use 500 m 
soils500<- soils %>% filter(DistanceClass_m == 500) %>%
  dplyr::select(PlotID ='UniquePlotID', RRC=`RRC_3classes_with name`, SRD=`SRD_3classes_with name`)

# now add the soils and ndvi 

eco.params %<>% left_join(soils500, by = "PlotID")
eco.params <- mutate(eco.params, Bare.inv = (sqrt(100 - BareSoil))) %>% dplyr::select(-BareSoil)

eco.params$Protein[eco.params$Protein == "NA"] <- mean(eco.params$Protein)

# TRANSFORMED DATA ***********************************************************
test<- eco.params %>% mutate_at(c(5:12, 14:15), funs(log(1+.)))
# ******************************************************************************
```

## Factor Analysis
results are essentially correlations between the variables and the factors (or "new" variables), as they are extracted by default; these correlations are also called factor loadings.
#### Checking the correlation matrix and sampling adequacy for different combinations of variables
First with the full set of vars
```{r}
# subset the full df to just get the ecol related vars:
ecols<- dplyr :: select(test, 5:19)
# create correlation matrix using hetcor function for moving on...
ecor <- hetcor(ecols)$cor
corrplot(ecor)
```
OK so it looks like we need to remove some vars that are really highly correlated.
This revised set looks better.
(see commented out code in chunk w old subsets and vals for comparison)
```{r}
# 9/21::
ecol <- dplyr::select(test, c(5,8,11,12,14:19)) 
ecor <- hetcor(ecol)$cor
corrplot(ecor)
```

Let's check sampling adequacy w this subset:
```{r}
k<- KMO(ecor)
kable(k$MSA, digits = 2, col.names = "Overall MSA")
kable(k$MSAi, digits = 2, col.names = "Individual/MSA")

```

Overall = 0.83. This is good enough to move forward w the CFA.
The test after the data were transformed/normalized retains the most number of variables and also has a good MSA. (Note, some vars were removed entirely bc not enough data, see notes in import)


### (draft)final list of paramaters to use going forward...

So, now the the data are transformed, we can pretty much keep everythig except maybe Annual Forb, which has the worst MSA score...
```{r}
# so now moving forward, the df to use is this one:
eco.vars<- test[,c(1:5, 8,11, 12, 14:19)]
# and the cor matrix is the one call above: ecor

```

### trying diff methods to compare outputs
```{r}
fa.parallel(ecor, n.obs = nrow(eco.vars), fm = "ml")$fa.values
```
This now suggests 2 factors....

Trying with fa() from the psych pkg, varying the num of factors proposed...
Get same result if vary teh number of proposed factors, all suggest one factor.
But very low Prop of Variance explained (with this full set of vars.)
```{r}
# with MaxLik:
faml <- fa(ecor, factors = 4, rotation = "varimax", fm = "ml", n.iter = 500, n.obs= nrow(eco.vars))

#faols <- fa(ecor, factors = 4, rotation = "varimax", fm = "ols")
#faminres <- fa(ecor, factors = 4, rotation = "varimax", fm = "minres")
faml
faml$loadings
faols$loadings 
faminres$loadings
#faminres$Structure
#fa1$residual
```
Which suggests that one factor works as well.
(Old version::But the proportion of variance explained is really low = 0.43_
Prop var explained is now = 0.52
What is going on with protein and bare.inv???


Trying w another approach, varying the number of factors that it forces, and comparing the variance explained...
usin gthe factanal() from the XXXX pkg: 
So tried 3:
```{r}
fa3<-factanal(~PerrGrass+ PerrForb + Sedges + Shrubs +Bare.inv + LitterCover + Protein + SpRich + RRC+ SRD, 
         covmat = ecor, n.obs = nrow(eco.vars),
         factors = 3, rotation = "varimax", fm = "ml")
fa3
```
Cum Var is 0.63. But the loadings across the factors isn't particularly informative.)

But 1 doesn't do a great job either.
```{r}
f1<- factanal(~PerrGrass+ PerrForb + Sedges + Shrubs +Bare.inv + LitterCover + Protein + SpRich + RRC+ SRD, 
         covmat = ecor, n.obs = nrow(eco.vars),
         factors = 1, rotation = "varimax", fm = "ml")#
f1
```

2 factors seems to make the most sense
```{r}
f2<- factanal(~PerrGrass+ PerrForb + Sedges + Shrubs +Bare.inv + LitterCover + Protein + SpRich + RRC+ SRD,
         covmat = ecor, n.obs = nrow(eco.vars),
         factors = 2, rotation = "varimax", fm = "ml")#$loadings
f2
```
Check out the difference in Uniquenesses between the runs w diff numb of factors. We want LOW uniqueness values (but not toooooo low--> Heywood). The values seem sort of midrange.... too high? 

Let's try it one more time w/o Protein, since it is neg.

Same 2 factor, but w/o protein.
F1 captures: Grass, Litter, RRC SRC Bare.inv
F2 captures: Forb, Sedge & SpRich
Shrubs is shared pretty much equally between the two-- kind of makes sense. May be representing two diff “kinds” of shrubs/two diff processes. lso has highest "uniqueness" value

```{r}
eco.vars.noP <- eco.vars %>% dplyr::select(-Protein)
tcor<- hetcor(eco.vars.noP[,5:13])$cor
tf2<- factanal(~PerrGrass+ PerrForb + Sedges + Shrubs +Bare.inv + LitterCover + SpRich + RRC+ SRD,
         covmat = tcor, n.obs = nrow(eco.vars),
         factors = 2, rotation = "varimax", fm = "ml")#$loadings
tf2

#  overwrite eco.vars to remove Protein
eco.vars <- eco.vars.noP
```





## summary so far
All of this is suggesting we just go with two latent Ecological Conditions.
[[Reminder -- original EFA to CFA analyses found similar fit w ecol data just normalized, not standardized by ecol zone, compared to standardized data. But, we could go back and do that again for these new vars if want to see how that affects this braoder list of vars]]

## CFA
See cfa.R for code from the original cfa runs and tests of measurement invariance.
See earlier versions in git for othe rpast analyses
See below this section for previous analysis on a single latent var 
holding here for reference.....modificationindices(fit.cfa3, sort. = TRUE)

#### New Measurement Models on 2 Latent Factors:

This one WITH shrubs:
```{r}
# USE THIS BELOW FOR QUICKLY COMPARING FIT MEASURES BTWN RUNS
fitmsrs <- c("df","chisq", "cfi", "rmsea", "rmsea.pvalue", "srmr")
# ecological indicator mesurement model:
#   specify the hypothesized model:  
#   this one WITH shrubs
twofact.mod <- 'eco1 =~ PerrGrass + LitterCover +RRC + SRD + Bare.inv + Shrubs
                eco2 =~ PerrForb +Sedges + SpRich + Shrubs'

#   fit the model
fit.ecol.mod <- cfa(twofact.mod, data= eco.vars, std.lv = TRUE)

summary(fit.ecol.mod, 
  #      rsquare = T, 
        standardized = T, 
        fit.measures=T)

```

Check the Modificatin Indices:
None of these seem particularly terrible. Could maybe link the soils params.
```{r}
modificationindices(fit.ecol.mod, sort. = TRUE)[1:6,]
```

This one with a specifies correlation btwn eco1 and eco 2:
Get the exact same values.
```{r, eval= FALSE}
# ecological indicator mesurement model:
#   specify the hypothesized model:  
#   this one WITH shrubs
twofactcor.mod <- 'eco1 =~ PerrGrass + LitterCover +RRC + SRD + Bare.inv + Shrubs
                eco2 =~ PerrForb +Sedges + SpRich + Shrubs
                eco1 ~~ eco2'

#   fit the model
fit.ecolcor.mod <- cfa(twofactcor.mod, data= eco.vars, std.lv = TRUE)

summary(fit.ecolcor.mod, 
  #      rsquare = T, 
        standardized = T, 
        fit.measures=T)
```
This one without Shrubs in either of the latents:
And the fit is poorer.
It makes more sense to leave Shrubs in. Can explain that they may be doing diff things in the two latents.

```{r, }
# ecological indicator mesurement model:
#   specify the hypothesized model:  
#   this one WITHOUT shrubs
twofactcorNoshrub.mod <- 'eco1 =~ PerrGrass + LitterCover +RRC + SRD + Bare.inv  
                eco2 =~ PerrForb +Sedges + SpRich 
                '

#   fit the model
fit.Noshrub.mod <- cfa(twofactcorNoshrub.mod, data= eco.vars, std.lv = TRUE)

# summary(fit.Noshrub.mod, 
#   #      rsquare = T, 
#         standardized = T, 
#         fit.measures=T)
fitMeasures(fit.Noshrub.mod, fit.measures = fitmsrs)

```


### Create a table with general summary of outputs.

## Check out the diagrams anyway:
```{r}
semPaths(fit.ecol.mod, 
         "std", 
         edge.label.cex = 0.95, 
         layout = "circle", 
         residuals = FALSE)
title("Two Factor Model Fit", line=1)
```

```{r}
semPaths(fit.Noshrub.mod, "std", 
         edge.label.cex = 0.95, 
         layout = "circle", 
         residuals = FALSE,
         intercepts = TRUE)
title("2 Factor CFA without Shrubs", line = 1)
```

## Eastimating predicted values:
Notes from Kelly's email:
Estimating predicted values – if I’m understanding your question correctly, I think you want the function `lavPredict()` (http://finzi.psych.upenn.edu/library/lavaan/html/lavPredict.html) with the default “type = “lv”. First you run a model with sem or cfa function, and then the fit of that model is what gets passed to this predict function. The output is a table with the predicted values for each of the rows in your original data. Then you can use those values to estimate the mean within each of the ecological zones or other categories. 

### New two factor model:
```{r}
#type	= A character string. If "lv", estimated values for the latent variables in the model are computed. If "ov", model predicted values for the indicators of the latent variables in the model are computed.

# predicting based on the model w two latent factors:
eco2.pred <- lavPredict(fit.ecol.mod, type = "lv", label = TRUE)
saveRDS(eco2.pred, file= "./data/eco.pred.TwoLatents.RDS")

```
Histograms of the two latent var outputs:
This doesn't mean a whole lot right now, per se. 
```{r}
hist(eco2.pred[,1])
hist(eco2.pred[,2])
summary(eco2.pred)
plot(ecdf(eco2.pred[,1]))
lines(ecdf(eco2.pred[,2]), col = "blue")
plot(density(eco2.pred[,1]), ylim = c(0,0.5))
lines(density(eco2.pred[,2]), col = "blue")
```

Rescale each to be 0-1? or 0-100? 
Should we base the rescaling of each on the max range of both vars, not within each one? 
```{r}
ecof1 <- eco2.pred[,1]
ecof2 <- eco2.pred[,2]
rescf1 <- scales::rescale(ecof1, to = c(0,1))
rescf2 <- scales::rescale(ecof2, to = c(0,1))

plot(ecdf(ecof1))
lines(ecdf(ecof2), col = "blue")
plot(density(ecof1), ylim = c(0, 0.5))
lines(density(ecof2), col = "blue")
```

OK, these both look similar after rescaling so I think we're ok to move ahead.



## THIS IS THE FIRST VERSION OF THIS, WITH A SMALLER SUBSET OF ECOL VARS AND JUST A SINGLE LATENT VAR:
A lot of the analyses that led up to this were deleted above. To pull those back in will have to check out an earlier version from git.
For now, this is saved also as an rds as "eco.pred.latent.RDS"
```{r}
#type	= A character string. If "lv", estimated values for the latent variables in the model are computed. If "ov", model predicted values for the indicators of the latent variables in the model are computed.

# predicting based on the unstandardized data:
eco.pred <- lavPredict(fit.eco.mod, type = "lv", label = TRUE)
saveRDS(eco.pred, file= "./data/eco.pred.latent.RDS")
# predicting based on the data standardized by ez:
ecol.pred <- lavPredict(fit.ecol.mod, type = "lv", label = TRUE) 
```
How much do these values differ?
Range is same but the distributions are maybe different?
```{r}
hist(eco.pred[,1])
title("Predicted Latent Ecological Condition for Each Household")

ggplot(data=eco.pred.df, aes(x=pred.eco))+
 # geom_histogram(fill = "#2b83ba", binwidth = 0.2)+
 # geom_density(fill = "#2b83ba")+
  geom_dotplot(fill = "#2b83ba")+
  geom_vline(xintercept = -0.4, linetype = 2)+
  labs(title = "Predicted Latent Ecological Condition for Each Household")+
  theme_bw()

hist(ecol.pred[,1])
```
Let's test them:
```{r}
ks.test(eco.pred[,1], ecol.pred[,1])
```
OK, so we can probably reasonably conclude that these latent vals are drawn from the same distribution??


Can we just assume that these values will map in the same order to the original df? how can we compare averages across groups, etc. if we can't be sure which HH they belong to?
### how to asign these back to the df w RefNums?
Will try just adding it to it for now....

```{r}
eco.lv <- as.data.frame(eco.pred[,1])
ecol.lv <- as.data.frame(ecol.pred[,1])
rpe.lv<- cbind(rpesoil, eco.lv, ecol.lv)
names(rpe.lv)[55:56] <- c("eco.lv", "eco.s.lv")
```

## let's play with plotting it just to see....
unstandardized:
```{r}
gez<- ggplot(rpe.lv, aes(CBRM_type, eco.lv))
gez + geom_boxplot()
gez + geom_violin()
gez + geom_violin()+
  facet_wrap(~ ez, labeller = label_both)

```
standardized:
```{r}
gez<- ggplot(rpe.lv, aes(ez, eco.s.lv))
gez + geom_violin()

```

## final notes::
These predicteed latent vals for ecological indicator will be used as a response variable in subsequent modelling, including BHM and the matching stuff.... 
so maybe we should play around with it a bit first to see if anything interesting shakes out...

# lm w the latent var
```{r, echo = FALSE}
# scatterplotMatrix(
#   formula = ~  pl + p2s + p3s + eco.lv |r1,
#   data = rpe.lv,
#   diagonal = "density",
#   by.groups = TRUE)


```


