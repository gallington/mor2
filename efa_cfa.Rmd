---
title: "efa_cfa"
output:
  pdf_document: default
  html_notebook: default
  html_document: default
---
```{r echo = FALSE, messages = FALSE}
library(foreign)
library(corrplot)
library(car)
library(ltm)
library(lavaan)
library(psych)
library(semTools)
library(semPlot)
library(magrittr)
library(tidyr)
library(dplyr)
library(ggplot2)
library(rgdal)
library(sp)
library(MatchIt)

```
Note: On 9/20 I removed all the extra code that is pulling variables from mor2 that aren't being used in the EFA process.  
All we really need here are the ecological parameters and the Ref Number IDs so we can link it back with the other data on practices, etc. later.

### Factor Analysis:
The main applications of factor analytic techniques are: (1) to reduce the number of variables and (2) to detect structure in the relationships between variables, that is to classify variables. Therefore, factor analysis is applied as a data reduction or structure detection method 




#### Info on the DATA: 

In order to execute this file you need to have already run the following:
1. hh_trends.R
2. soil_params.R

### Data screening:

```{r echo=FALSE}
mor2<- foreign::read.spss("./data/ALL_EcolHHOrgMerged_byUvuljaa_03_15_17_Simplified.sav" , 
                 to.data.frame=TRUE,
                 use.value.labels=FALSE) #
# tidy format
tbl_df(mor2)


eco.params <- dplyr :: select(mor2, 
            # identifiers for linking w other datasets:
             RefNum = SocialSurveyReferenceNumber, 
             PlotID = UniquePlotID500,  #this one incorps the Uvuulja and site
             Lat = Latitude500, # bc need to match later at 500
             Long= Longitude500,
            # grasses:
             PerrGrass = PerGrassCover_percent_Mean500_1000,
             Grass_bm = Grass_gm2_Mean500_1000,
            # AnuGrass = AnGrassCover_percent_Mean500_1000,  #  not enough of this to use it. Mostly zeros....
             TotGrassCover = TotalGrassCover_percent_Mean500_1000,
            # Acnath = Acnath_gm2_Mean500_1000,  # almost allll zeros
            # forbs:
             PerrForb = PerForbCover_percent_Mean500_1000,
             AnuForb = AnForbCover_percent_Mean500_1000,
             Forb_bm = Forb_gm2_Mean500_1000,
            # sedges:
             Sedges = TotalSedgeCover_percent_Mean500_1000,
            # shrubs:
             Shrubs = TotalShrubCover_percent_Mean500_1000,
             TotFoliar = TotalFoliarCover_percent_Mean500_1000,
            # BasalCover = BasalCover_percent_Mean500_1000,   # something is wrong w this variable. values are huge and tiny.
            # abiotic params:
             BareSoil = BareSoil_percent_Mean500_1000,
             LitterCover = LitterCover_percent_Mean500_1000,
             Protein = CrudeProtein_percent_Mean500_1000,
            # species richness:
             SpRich = SpeciesRichnessMean500_1000
             )


#NO. BC we end up useing these ppt and ndvi vars later when matching, so we cannot also use them to decribe this as well.

# ndvi values exported from GEE:
# replace these w Jay's values when we get them:
# pttrends <- read.csv("./data/pttrends.csv", stringsAsFactors = FALSE)
# #pttrends <- readOGR(dsn = "./data", layer = "pttrends.csv")
# pttrends<- pttrends[1:130, 1:5]
# 
# geo<- pttrends$.geo
# 
# var<- sapply(pttrends$.geo, function(x) strsplit(x, split="[", fixed = TRUE))
# coord<- sapply(var, function(x) x[2])
# names(coord)<- NULL
# 
# coord<- sapply(coord, function(x) substr(x, 1, nchar(x)-2))
# split<- sapply(coord, function(x) strsplit(x, split = ","))
# pttrends$long <- sapply(split, function(x) x[1])
# pttrends$lat <- sapply(split, function(x) x[2])
# 
# preciptrend <- dplyr::select(pttrends, -.geo)
# preciptrend %<>% mutate_at(5:6, funs(as.numeric(.))) %>%
#   rename(long2 = long, lat2 = lat)

# WHY ARE THERE ALL THESE NEGATIVE VALUES???????????
# DO NOT USE MEAN OR SLOPE.


# add in the soils parameters

#library(readxl)
soils <- read_excel("/nfs/gallington-data/Herder_sem/data/Soil surface data_Ginger.xlsx", 
                    sheet = "Sheet1")
#View(soils)
soils %<>% mutate_at(9:18, funs(factor(.)))

# do these need to be flipped? so best condition is highest?
erosion.levels<- c("high erosion", "medium erosion", "little erosion")
retention.levels <- c("isolated", "fragmented", "connected")
soils$`SRD_3classes_with name` <- ordered(soils$`SRD_3classes_with name`, levels = erosion.levels)
soils$`RRC_3classes_with name` <- ordered(soils$`RRC_3classes_with name`, levels = retention.levels)
# can't average bc they are categorical so chose to use 500 m 
soils500<- soils %>% filter(DistanceClass_m == 500) %>%
  dplyr::select(PlotID ='UniquePlotID', RRC=`RRC_3classes_with name`, SRD=`SRD_3classes_with name`)

# now add the soils and ndvi 

eco.params %<>% left_join(soils500, by = "PlotID")
eco.params <- mutate(eco.params, Bare.inv = (sqrt(100 - BareSoil))) %>% dplyr::select(-BareSoil)

eco.params$Protein[eco.params$Protein == "NA"] <- mean(eco.params$Protein)

# TRANSFORMED DATA ***********************************************************
test<- eco.params %>% mutate_at(c(5:12, 14:15), funs(log(1+.)))
# ******************************************************************************
```

## Factor Analysis
results are essentially correlations between the variables and the factors (or "new" variables), as they are extracted by default; these correlations are also called factor loadings.
#### Checking the correlation matrix and sampling adequacy for different combinations of variables
First with the full set of vars
```{r}
# subset the full df to just get the ecol related vars:
ecols<- dplyr :: select(test, 5:19)
# create correlation matrix using hetcor function for moving on...
ecor <- hetcor(ecols)$cor
corrplot(ecor)
```
OK so it looks like we need to remove some vars that are really nighly correlated.

```{r}
# 9/21::
ecol <- dplyr::select(test, c(5,8,11,12,14:19))

# 9/20:
eco1<- dplyr :: select(eco.params, c(5,8, 13, 14, 17:22))  # MSA = 0.81
eco2<- dplyr :: select(eco.params, c(5,8, 14, 17:22)) # MSA = 0.84
# put forb back in?
eco3<- dplyr :: select(eco.params, c(5,8,10, 14, 17:22)) # MSA= 0.74
# original set of vars:
eco0<- dplyr :: select(eco.params, c(5,10, 17, 22))  # MSA = 0.59
ecor <- hetcor(ecol)$cor
corrplot(ecor)
```

Let's check sampling adequacy w this subset:
Overall = 0.83. This is good enough to move forward w the CFA.
```{r}
# Kaiser (1975) suggested that KMO > .9 were marvelous, in the .80s, mertitourious, in the .70s, middling, in the .60s, medicore, in the 50s, miserable, and less than .5, unacceptable.
k<- KMO(ecor)   

kable(k$MSA, digits = 2, col.names = "Overall MSA")
kable(k$MSAi, digits = 2, col.names = "Individual/MSA")
```

```{r}


# create correlation matrix using hetcor function for moving on...
ecor <- hetcor(ecols)$cor

# Assessing sampling adequacy:
# Kaiser (1975) suggested that KMO > .9 were marvelous, in the .80s, mertitourious, in the .70s, middling, in the .60s, medicore, in the 50s, miserable, and less than .5, unacceptable.
k<- KMO(ecor)   
kable(k$MSA, digits = 2, col.names = "Overall MSA")
kable(k$MSAi, digits = 2, col.names = "Individual/MSA")

cortest.bartlett(ecor, n = 130)
```
So, with all the ecological parameters together, we get an MSA of 0.24 = Unacceptable.
We can then assess each individual parameter to see which we might need to remove from consideration bc the sample size is too low.
It looks like a lot shoudl be removed.  But we had much better MSA in earlier iterations where we were using smaller number of params. So, let's remove a few of the ones we've just added, to see how this affects the score.

The test after the data were transformed/normalized retains the most number of variables and also has a good MSA. (Note, some vars were removed entirely bc not enough data, see notes in import)


### (draft)final list of paramaters to use going forward...

So, now the the data are transformed, we can pretty much keep everythig except maybe Annual Forb, which has the worst MSA score...
```{r}
#eco.vars<- eco.params[,c(5, 8, 13, 16:22)]

# with normalized data, removing Annual Forb:
test2<- dplyr::select(test, -AnuForb)
eco.vars<- test2[,5:18]  # pulling out just the eco dat to create corr matrix
# create correlation matrix using hetcor function for moving on...
ecor <- hetcor(eco.vars)$cor
```



## with pos/neg indicators combined
### trying diff methods to compare outputs
```{r}
fa.parallel(ecor, n.obs = nrow(eco.vars), fm = "ml")$fa.values
```
This now suggests 3 factor....

with fa() from the psych pkg
```{r}
faml <- fa(ecor, factors = 4, rotation = "varimax", fm = "ml", n.iter = 500, n.obs= nrow(eco.vars))
# get same result if vary teh number of proposed factors...
faols <- fa(ecor, factors = 4, rotation = "varimax", fm = "ols")
faminres <- fa(ecor, factors = 4, rotation = "varimax", fm = "minres")
faml
faml$loadings
faols$loadings 
faminres$loadings
#faminres$Structure
#fa1$residual
```
Which suggests that one factor works as well.
(Old version::But the proportion of variance explained is really low = 0.43_
Prop var explained is now = 0.52
What is going on with protein and bare.inv???


Trying w another approach, varying the number of factors that it forces, and comparing the variance explained...
usin gthe factanal() from the XXXX pkg: 
So tried 3:
```{r}
factanal(~PerrGrass+ TotGrassCover + PerrForb + Forb_bm+ Sedges +Bare.inv + LiiterCover + Protein + SpRich + RRC+ SRD, 
         covmat = ecor, n.obs = nrow(eco.vars),
         factors = 3, rotation = "varimax", fm = "ml")$loadings
```
(OLD VERSION:Cum Var is 0.664. But the loadings across the factors isn't particularly informative.)
New Version: Cum Var is 0.687on 3 factors... but a little hard to interpret...

```{r}
f1<- factanal(~PerrGrass+ Grass_bm+ TotGrassCover + PerrForb + Forb_bm+ Sedges +Bare.inv + LitterCover + Protein + SpRich + RRC+ SRD, 
         covmat = ecor, n.obs = nrow(eco.vars),
         factors = 1, rotation = "varimax", fm = "ml")#
f1
```
Check out the difference in Uniquenesses between the runs w diff numb of factors. We want LOW uniqueness values (but not toooooo low--> Heywood)
```{r}
f2<- factanal(~PerrGrass + Grass_bm + TotGrassCover + PerrForb +     Forb_bm + Sedges + Bare.inv + LitterCover + Protein + SpRich +     RRC + SRD,
         covmat = ecor, n.obs = nrow(eco.vars),
         factors = 2, rotation = "varimax", fm = "ml")#$loadings
f2
```



```{r}
f3<- factanal(~PerrGrass + Grass_bm + TotGrassCover + PerrForb +     Forb_bm + Sedges + Bare.inv + LitterCover + Protein + SpRich +     RRC + SRD,
         covmat = ecor, n.obs = nrow(eco.vars),
         factors = 3, rotation = "varimax", fm = "ml")#$loadings
f4<- factanal(~PerrGrass + Grass_bm + TotGrassCover + PerrForb +     Forb_bm + Sedges + Bare.inv + LitterCover + Protein + SpRich +     RRC + SRD,
         covmat = ecor, n.obs = nrow(eco.vars),
         factors = 4, rotation = "varimax", fm = "ml")#$loadings
```
OK, there's just too much overlap in some of these vars. Let's cut it down again:

```{r}
corrplot(ecor)
trim<- test[, c(1:5,8,9,  11, 12,  14:19 )]

tcor<- hetcor(trim[,5:15])$cor
corrplot(tcor)

```
```{r}
ft<- factanal(~PerrGrass + PerrForb +   AnuForb + Sedges + Bare.inv + LitterCover + Protein + SpRich +     RRC + SRD,
         covmat = tcor, n.obs = nrow(eco.vars),
         factors = 2, rotation = "varimax", fm = "ml")#$loadings
ft
```



OK, need to figure out what is going on with Protein and Bare Ground..................


















## POSITIVE INDICATORS 
```{r}
#  have to resubset to just get the indicators of integrity
ecols.pos<- dplyr :: select(rpesoil, g.stand, l.stand, bare.inv, RRC, ndvi)

# create correlation matrix using hetcor function for moving on...
ecor.pos <- hetcor(ecols.pos)$cor
```
Now factor analysis from this:
```{r}
fa2 <- fa(ecor.pos, factors = 4, rotation = "varimax", fm = "ml")
summary(fa2)
fa2$loadings
#fa2$residual
```
This method suggests that just one factor is sufficient. GOOD!
And with alternate version:

```{r}
factanal(~g.stand+ f.stand+ bare.inv+ l.stand+ RRC+ SRD+ ndvi, 
         covmat = ecor.pos, n.obs = nrow(ecols),
         factors = 1, rotation = "varimax", fm = "ml")$loadings
```
trying with specifying 2 factors, just to see:
```{r}
factanal(~g.stand+ f.stand+ bare.inv+ l.stand+ RRC+ SRD+ ndvi, 
         covmat = ecor, n.obs = nrow(ecols),
         factors = 2, rotation = "varimax", fm = "ml")$loadings
```
Only get slightly higher cummulative variance and loadings are hard to decipher.

### EFA W NEGATIVE INDICATORS...

```{r}
#  have to resubset to just get the indicators of integrity
ecols.neg<- dplyr :: select(rpesoil, f.stand, SRD)

# create correlation matrix using hetcor function for moving on...
ecor.neg <- hetcor(ecols.neg)$cor
```
Now factor analysis from this:
```{r}
fa2n <- fa(ecor.neg, factors = 4, rotation = "varimax", fm = "ml")
summary(fa2n)
fa2n$loadings
#fa2$residual
```

## summary so far
All of this is suggesting we just go with a single latent Ecological Condition construct.
Looks like we will just keep RRC and SRD together.
But even still the prop of variance explained is pretty low?? 
The forbs are messing everything up?

*Just one more thing:*
## Trying again with original data, not standaradized by ecol zone...
Also removed forbs, bc they just aren't lining up.
```{r}
# first have to flip e3 (which is % bare ground) to match the directionality of the others

rpesoil %<>% mutate(e3.inv = (100-e3))
ecol.vars<- dplyr :: select(rpesoil,e1, e3.inv, e4, RRC, SRD, ndvi)

# create correlation matrix using hetcor function for moving on...
ecor.orig.vars <- hetcor(ecol.vars)$cor
# and a quick factor analysis w these vars:
fa.ov <- fa(ecor.orig.vars, factors = 4, rotation = "varimax", fm = "ml")
summary(fa.ov)
fa.ov$loadings

```
 Still holding strong w one factor. 
 62.8% of variance explained. Best yet?
 Eventually we'd want to flip the bare (e3) to fix the sign.

 
Let's compare that to the standardized vars, and remove forbs also.
I'm just calling this the same thing and writing over it
```{r}
# subset the full df to just get the ecol related vars:
ecols<- dplyr :: select(rpesoil, g.stand, bare.inv, l.stand, RRC, SRD, ndvi)

# create correlation matrix using hetcor function for moving on...
ecor <- hetcor(ecols)$cor
# and EFA it:
fa <- fa(ecor, factors = 2, rotation = "varimax", fm = "ml")
summary(fa)
fa$loadings

```
Poorer fit with these...................................
???? SO, use orig vars? Those are normalized but not standardized to ez.

## CFA
# or: ok, let's just go back to CFA w this new configuration....
See cfa.R for code from the original cfa runs and tests of measurement invariance.
holding here for reference.....modificationindices(fit.cfa3, sort. = TRUE)
```{r}
# USE THIS BELOW FOR QUICKLY COMPARING FIT MEASURES BTWN RUNS
fitmsrs <- c("df","chisq", "cfi", "rmsea", "rmsea.pvalue", "srmr")
#but, these aren't based on the standardized

# ecological indicator mesurement model:
ecol.mod <- 'eco =~ g.stand+ bare.inv + l.stand + RRC+ SRD+ ndvi'
fit.ecol.mod <- cfa(ecol.mod, data= rpesoil, std.lv = TRUE)
eco.mod <- 'eco =~ e1 + e3 + e4 + RRC+ SRD+ ndvi'
fit.eco.mod <- cfa(eco.mod, data= rpesoil, std.lv = TRUE)
summary(fit.ecol.mod, 
  #      rsquare = T, 
        standardized = T, 
        fit.measures=T)
summary(fit.eco.mod, 
  #     rsquare = T, 
        standardized = T, 
        fit.measures=T)

```
Let's check the modification indices too:
```{r}
modificationindices(fit.ecol.mod, sort. = TRUE) # with the standardized ecol vars
modificationindices(fit.eco.mod, sort. = TRUE) # not standardized, orgi normalized msrmnts
```
Making edits to the model based on the modification indices:

```{r}
# ecological indicator mesurement model:
ecol.mod <- 'eco =~ g.stand+ bare.inv + l.stand + RRC+ SRD+ ndvi
             l.stand ~~ RRC  
            '
fit.ecol.mod <- cfa(ecol.mod, data= rpesoil, std.lv = TRUE)
eco.mod <- 'eco =~ e1 + e3.inv + e4 + RRC+ SRD+ ndvi
       #     e3.inv ~~ ndvi
            '
fit.eco.mod <- cfa(eco.mod, data= rpesoil, std.lv = TRUE)
summary(fit.ecol.mod, 
  #      rsquare = T, 
        standardized = T, 
        fit.measures=T)
summary(fit.eco.mod, 
  #     rsquare = T, 
        standardized = T, 
        fit.measures=T)
```
### Ok, so.... how do we decide which is better???
With or without the standardized??? 
Or is the message that we are ok to go with either?

### Create a table with general summary of outputs.

## Check out the diagrams anyway:
```{r}
semPaths(fit.ecol.mod, "std", edge.label.cex = 0.75, layout = "tree", residuals = FALSE)
title("stnd_ez", line=1)
```

```{r}
semPaths(fit.eco.mod, "std", 
         edge.label.cex = 0.95, 
         layout = "circle", 
         residuals = FALSE,
         intercepts = TRUE)
title("CFA of Latent Ecological Condition", line = 1)
```

## Eastimating predicted values:
Notes from Kelly's email:
Estimating predicted values – if I’m understanding your question correctly, I think you want the function `lavPredict()` (http://finzi.psych.upenn.edu/library/lavaan/html/lavPredict.html) with the default “type = “lv”. First you run a model with sem or cfa function, and then the fit of that model is what gets passed to this predict function. The output is a table with the predicted values for each of the rows in your original data. Then you can use those values to estimate the mean within each of the ecological zones or other categories. 

```{r}
#type	= A character string. If "lv", estimated values for the latent variables in the model are computed. If "ov", model predicted values for the indicators of the latent variables in the model are computed.

# predicting based on the unstandardized data:
eco.pred <- lavPredict(fit.eco.mod, type = "lv", label = TRUE)
# predicting based on the data standardized by ez:
ecol.pred <- lavPredict(fit.ecol.mod, type = "lv", label = TRUE) 
```
How much do these values differ?
Range is same but the distributions are maybe different?
```{r}
hist(eco.pred[,1])
title("Predicted Latent Ecological Condition for Each Household")

ggplot(data=eco.pred.df, aes(x=pred.eco))+
 # geom_histogram(fill = "#2b83ba", binwidth = 0.2)+
 # geom_density(fill = "#2b83ba")+
  geom_dotplot(fill = "#2b83ba")+
  geom_vline(xintercept = -0.4, linetype = 2)+
  labs(title = "Predicted Latent Ecological Condition for Each Household")+
  theme_bw()

hist(ecol.pred[,1])
```
Let's test them:
```{r}
ks.test(eco.pred[,1], ecol.pred[,1])
```
OK, so we can probably reasonably conclude that these latent vals are drawn from the same distribution??


Can we just assume that these values will map in the same order to the original df? how can we compare averages across groups, etc. if we can't be sure which HH they belong to?
### how to asign these back to the df w RefNums?
Will try just adding it to it for now....

```{r}
eco.lv <- as.data.frame(eco.pred[,1])
ecol.lv <- as.data.frame(ecol.pred[,1])
rpe.lv<- cbind(rpesoil, eco.lv, ecol.lv)
names(rpe.lv)[55:56] <- c("eco.lv", "eco.s.lv")
```

## let's play with plotting it just to see....
unstandardized:
```{r}
gez<- ggplot(rpe.lv, aes(CBRM_type, eco.lv))
gez + geom_boxplot()
gez + geom_violin()
gez + geom_violin()+
  facet_wrap(~ ez, labeller = label_both)

```
standardized:
```{r}
gez<- ggplot(rpe.lv, aes(ez, eco.s.lv))
gez + geom_violin()

```

## final notes::
These predicteed latent vals for ecological indicator will be used as a response variable in subsequent modelling, including BHM and the matching stuff.... 
so maybe we should play around with it a bit first to see if anything interesting shakes out...

# lm w the latent var
```{r, echo = FALSE}
# scatterplotMatrix(
#   formula = ~  pl + p2s + p3s + eco.lv |r1,
#   data = rpe.lv,
#   diagonal = "density",
#   by.groups = TRUE)


```


