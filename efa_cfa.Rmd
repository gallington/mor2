---
title: "efa_cfa"
output:
  pdf_document: default
  html_notebook: default
  html_document: default
---
```{r setup, echo = FALSE, message = FALSE, include = FALSE, warning= FALSE}
library(foreign)
library(corrplot)
library(car)
library(ltm)
library(lavaan)
library(psych)
library(semTools)
library(semPlot)
library(magrittr)
library(tidyr)
library(dplyr)
library(ggplot2)
library(rgdal)
library(sp)
library(readxl)
library(knitr)
```
Note: on 12/11 I removed all extra code that wasn't used in the final analysis used in the ms submission. 

Note: On 9/20 I removed all the extra code that is pulling variables from mor2 that aren't being used in the EFA process.  All we really need here are the ecological parameters and the Ref Number IDs so we can link it back with the other data on practices, etc. later.

### Factor Analysis:
The main applications of factor analytic techniques are: (1) to reduce the number of variables and (2) to detect structure in the relationships between variables, that is to classify variables. Therefore, factor analysis is applied as a data reduction or structure detection method 


#### Info on the DATA: 

In order to execute this file you need to have the following additional files:
1. ALL_EcolHHOrgMerged_byUvuljaa_03_15_17_Simplified.sav
2. Soil surface data_Ginger.xlsx

### Data screening:

```{r echo=FALSE, chache = TRUE, results = 'hide' }
mor2<- foreign::read.spss("./data/ALL_EcolHHOrgMerged_byUvuljaa_03_15_17_Simplified.sav" , 
                 to.data.frame=TRUE,
                 use.value.labels=FALSE) #
# tidy format
tbl_df(mor2)


eco.params <- dplyr :: select(mor2, 
          # identifiers for linking w other datasets:
             RefNum = SocialSurveyReferenceNumber, 
             PlotID = UniquePlotID500,  #this one incorps the Uvuulja and site
             Lat = Latitude500, # bc need to match later at 500
             Long= Longitude500,
          # grasses:
             PerrGrass = PerGrassCover_percent_Mean500_1000,
             Grass_bm = Grass_gm2_Mean500_1000,
            # AnuGrass = AnGrassCover_percent_Mean500_1000,  #  not enough of this to use it. Mostly zeros....
             TotGrassCover = TotalGrassCover_percent_Mean500_1000,
            # Acnath = Acnath_gm2_Mean500_1000,  # almost allll zeros
          # forbs:
             PerrForb = PerForbCover_percent_Mean500_1000,
             AnuForb = AnForbCover_percent_Mean500_1000,
             Forb_bm = Forb_gm2_Mean500_1000,
          # sedges:
             Sedges = TotalSedgeCover_percent_Mean500_1000,
          # shrubs:
             Shrubs = TotalShrubCover_percent_Mean500_1000,
             TotFoliar = TotalFoliarCover_percent_Mean500_1000,
          # BasalCover = BasalCover_percent_Mean500_1000,   # something is wrong w this variable. values are huge and tiny.
          # abiotic params:
             BareSoil = BareSoil_percent_Mean500_1000,
             LitterCover = LitterCover_percent_Mean500_1000,
             Protein = CrudeProtein_percent_Mean500_1000,
          # species richness:
             SpRich = SpeciesRichnessMean500_1000
             )

# add in the soils parameters

#library(readxl)
soils <- read_excel("/nfs/gallington-data/Herder_sem/data/Soil surface data_Ginger.xlsx", 
                    sheet = "Sheet1")
#View(soils)
soils %<>% mutate_at(9:18, funs(factor(.)))

# do these need to be flipped? so best condition is highest?
erosion.levels<- c("high erosion", "medium erosion", "little erosion")
retention.levels <- c("isolated", "fragmented", "connected")
soils$`SRD_3classes_with name` <- ordered(soils$`SRD_3classes_with name`, levels = erosion.levels)
soils$`RRC_3classes_with name` <- ordered(soils$`RRC_3classes_with name`, levels = retention.levels)
# can't average bc they are categorical so chose to use 500 m 
# checked data and there were no many diffs between category at 500m and 1000m. biggest diff btwn 100m and 500/1000m
soils500<- soils %>% filter(DistanceClass_m == 500) %>%
  dplyr::select(PlotID ='UniquePlotID', RRC=`RRC_3classes_with name`, SRD=`SRD_3classes_with name`)

# now add the soils  

eco.params %<>% left_join(soils500, by = "PlotID")
eco.params <- mutate(eco.params, Bare.inv = (sqrt(100 - BareSoil))) %>% dplyr::select(-BareSoil)

eco.params$Protein[eco.params$Protein == "NA"] <- mean(eco.params$Protein)

# TRANSFORMED DATA ***********************************************************
test<- eco.params %>% mutate_at(c(5:12, 14:15), funs(log(1+.)))
# ******************************************************************************
```

## Factor Analysis
results are essentially correlations between the variables and the factors (or "new" variables), as they are extracted by default; these correlations are also called factor loadings.
#### Checking the correlation matrix and sampling adequacy for different combinations of variables
First with the full set of vars
```{r, echo= FALSE, fig.height=3, fig.width = 3}
# subset the full df to just get the ecol related vars:
ecols<- dplyr :: select(test, 5:19)
# create correlation matrix using hetcor function for moving on...
ecor <- hetcor(ecols)$cor
corrplot(ecor)
```
OK so it looks like we need to remove some vars that are really highly correlated.
This revised set looks better.
(see commented out code in chunk w old subsets and vals for comparison)
```{r echo= FALSE, fig.height= 3, fig.width = 3}
# 9/21::
ecol <- dplyr::select(test, c(5,8,11,12,14:19)) #subset of ecol vars
ecor <- hetcor(ecol)$cor
corrplot(ecor)
```

Let's check sampling adequacy w this subset:
```{r, echo= FALSE}
k<- KMO(ecor)
kable(k$MSA, digits = 2, col.names = "Overall MSA")
kable(k$MSAi, digits = 2, col.names = "Individual/MSA")

```

Overall = 0.83. This is good enough to move forward w the CFA.
The test after the data were transformed/normalized retains the most number of variables and also has a good MSA. (Note, some vars were removed entirely bc not enough data, see notes in import)


### (draft)final list of paramaters to use going forward...

So, now the the data are transformed, we can pretty much keep everythig except maybe Annual Forb, which has the worst MSA score...
```{r echo= FALSE}
# so now moving forward, the df to use is this one:
eco.vars<- test[,c(1:5, 8,11, 12, 14:19)]
# and the cor matrix is the one called above: ecor

```

### trying diff methods to compare outputs
```{r echo= FALSE, fig.height= 2, fig.width = 3}
fa.parallel(ecor, n.obs = nrow(eco.vars), fm = "ml")$fa.values
```
This now suggests 2 factors....

Trying with fa() from the psych pkg, varying the num of factors proposed...
Get same result if vary teh number of proposed factors, all suggest one factor.
But very low Prop of Variance explained (with this full set of vars.)
```{r, echo = FALSE, message = FALSE}
# with MaxLik:
faml <- fa(ecor, factors = 4, rotation = "varimax", fm = "ml", n.iter = 500, n.obs= nrow(eco.vars))

#faols <- fa(ecor, factors = 4, rotation = "varimax", fm = "ols")
#faminres <- fa(ecor, factors = 4, rotation = "varimax", fm = "minres")
#faml
faml$loadings
# faols$loadings 
# faminres$loadings
#faminres$Structure
#fa1$residual
```
Which suggests that one factor works as well.
(Old version::But the proportion of variance explained is really low = 0.43_
Prop var explained is now = 0.52
What is going on with protein and bare.inv???


Trying w another approach, varying the number of factors that it forces, and comparing the variance explained...
usin gthe factanal() from the XXXX pkg: 
So tried 3:
```{r, echo = FALSE}
fa3<-factanal(~PerrGrass+ PerrForb + Sedges + Shrubs +Bare.inv + LitterCover + Protein + SpRich + RRC+ SRD, 
         covmat = ecor, n.obs = nrow(eco.vars),
         factors = 3, rotation = "varimax", fm = "ml")
fa3
```
Cum Var is 0.63. But the loadings across the factors isn't particularly informative.)

But 1 doesn't do a great job either.
```{r, echo = FALSE}
f1<- factanal(~PerrGrass+ PerrForb + Sedges + Shrubs +Bare.inv + LitterCover + Protein + SpRich + RRC+ SRD, 
         covmat = ecor, n.obs = nrow(eco.vars),
         factors = 1, rotation = "varimax", fm = "ml")#
f1
```

2 factors seems to make the most sense
```{r, echo = FALSE}
f2<- factanal(~PerrGrass+ PerrForb + Sedges + Shrubs +Bare.inv + LitterCover + Protein + SpRich + RRC+ SRD,
         covmat = ecor, n.obs = nrow(eco.vars),
         factors = 2, rotation = "varimax", fm = "ml")#$loadings
f2
```
Check out the difference in Uniquenesses between the runs w diff numb of factors. We want LOW uniqueness values (but not toooooo low--> Heywood). The values seem sort of midrange.... too high? 

Let's try it one more time w/o Protein, since it is neg.

## FINAL SET OF VARS:

Same 2 factor structure, but w/o protein.
F1 captures: Grass, Litter, RRC SRC Bare.inv
F2 captures: Forb, Sedge & SpRich
Shrubs is shared pretty much equally between the two-- kind of makes sense. May be representing two diff “kinds” of shrubs/two diff processes. 
Also has highest "uniqueness" value
Not code chunk below writes over eco.vars to remove protein 
```{r, echo = FALSE}
eco.vars.noP <- eco.vars %>% dplyr::select(-Protein)
tcor<- hetcor(eco.vars.noP[,5:13])$cor
tf2<- factanal(~PerrGrass+ PerrForb + Sedges + Shrubs +Bare.inv + LitterCover + SpRich + RRC+ SRD,
         covmat = tcor, n.obs = nrow(eco.vars),
         factors = 2, rotation = "varimax", fm = "ml")#$loadings
tf2
xtable(tf2)
#  overwrite eco.vars to remove Protein from all future analyses.
eco.vars <- eco.vars.noP
```

Go with removing Protein from subsequent analyses.
eco.vars is the object to use going forward.

## summary so far
All of this is suggesting we just go with two latent Ecological Conditions.
[[Reminder -- original EFA to CFA analyses found similar fit w ecol data just normalized, not standardized by ecol zone, compared to standardized data. But, we could go back and do that again for these new vars if want to see how that affects this braoder list of vars]]

## CFA
See cfa.R for code from the original cfa runs and tests of measurement invariance.
See earlier versions in git for other past analyses
See below this section for previous analysis on a single latent var 
holding here for reference.....modificationindices(fit.cfa3, sort. = TRUE)

### New Measurement Models on 2 Latent Factors:

This one WITH shrubs is the better one, see past version for model w/o shrubs on both (poorer fit), and a version with specified covar btwn eco1&2, (which yields same results so dropped):
```{r, echo = FALSE, message = FALSE}
# USE THIS BELOW FOR QUICKLY COMPARING FIT MEASURES BTWN RUNS
fitmsrs <- c("df","chisq", "cfi", "rmsea", "rmsea.pvalue", "srmr")
# ecological indicator mesurement model:
#   specify the hypothesized model:  
#   this one WITH shrubs
twofact.mod <- 'eco1 =~ PerrGrass + LitterCover +RRC + SRD + Bare.inv + Shrubs
                eco2 =~ PerrForb +Sedges + SpRich + Shrubs
                '

#   fit the model
fit.ecol.mod <- cfa(twofact.mod, data= eco.vars, std.lv = TRUE)

summary(fit.ecol.mod, 
        rsquare = T, 
        standardized = T, 
        fit.measures=T)

```

Check the Modificatin Indices:
None of these seem particularly terrible. Could maybe link the soils params.
```{r, echo = FALSE, message=FALSE}
modificationindices(fit.ecol.mod, sort. = TRUE)[1:6,]
```


### Create a table with general summary of outputs.
(this never happened i guess)

## Check out the diagrams anyway:
```{r, echo = FALSE, fig.height= 3, fig.width=5}
semPaths(fit.ecol.mod, 
        what = "path",
         whatLabels ="std", 
         edge.label.cex = 0.75, 
         layout = "tree", 
         residuals = TRUE,
         esize = 2
         )
title("Two Factor Model Fit", line=1)
```

## Estimating predicted values:
Predicting values of the latent factors at each sampling site.
The output is a table with the predicted values for each of the rows in your original data. Then you can use those values to estimate the mean within each of the ecological zones or other categories. 

```{r, echo = FALSE}
#type	= A character string. If "lv", estimated values for the latent variables in the model are computed. If "ov", model predicted values for the indicators of the latent variables in the model are computed.

# predicting based on the model w two latent factors:
eco2.pred <- lavPredict(fit.ecol.mod, type = "lv", label = TRUE)
saveRDS(eco2.pred, file= "./data/eco.pred.TwoLatents.RDS")


```
Histograms of the two latent var outputs:

```{r, echo = FALSE, fig.height= 2.5, fig.width= 3.5}
hist(eco2.pred[,1])
hist(eco2.pred[,2])
summary(eco2.pred)
plot(ecdf(eco2.pred[,1]))
lines(ecdf(eco2.pred[,2]), col = "blue")
plot(density(eco2.pred[,1]), ylim = c(0,0.5))
lines(density(eco2.pred[,2]), col = "blue")
```

Rescale each to be 0-1? or 0-100? 
Should we base the rescaling of each on the max range of both vars, not within each one? 
### Rescaled latent factors
```{r, echo = FALSE, fig.height= 2.5, fig.width= 3.5}
ecof1 <- eco2.pred[,1]
ecof2 <- eco2.pred[,2]
rescf1 <- scales::rescale(ecof1, to = c(0,1), from = c(min(eco2.pred), max(eco2.pred)))
rescf2 <- scales::rescale(ecof2, to = c(0,1), from = c(min(eco2.pred), max(eco2.pred)))

plot(ecdf(rescf1))
lines(ecdf(rescf2), col = "blue")
plot(density(rescf1), ylim = c(0, 1))
lines(density(rescf2), col = "blue")
```

OK, these both look similar after rescaling so I think we're ok to move ahead.
Export these values for use in next analysis. 
```{r}
saveRDS(eco2.pred, "./data/eco2.pred.RDS")


# and merge it w the df:
eco.vars.pred <- cbind(test[,1:2], as.data.frame(eco2.pred))

saveRDS(eco.vars.pred, "./data/eco.2latent.RDS")
```










TO BE CUT ALSO ON 12/11: keeping for a minute just to look at it.
# SEM
Developing a structural model based on hypothesized relationships between practices, ecological factors
For now, borrowing code from cfa.R, which is the first iteration of these analyses. Using the results from that work that suggested a 3-factor model of practices was the best fit to the data.
See earlier work on EFA of practices to get info on how we arrived on these groupings.
```{r}
prac.3fmod <- ' seas.migr =~ 1*p3s + p2s + p10
          otor =~ 1*p4 + p5 + p9
          resv.past =~ 1*p7 + p6 + p8 + pl'

fit.prac3f<- cfa(prac.3fmod, data = raw.data, std.all = TRUE)
summary(fit.prac3f, 
        rsquare = T, 
        standardized = T, 
        fit.measures=T)

semPaths(fit.prac3f, nCharNodes= 0, whatLabels = "std", layout= "tree", residuals = FALSE)


```
DEveloping a structural model from here:
```{r}
# merge the datasets:
test$RefNum<- as.factor(test$RefNum)
sem.df<- left_join(raw.data, test, by= "RefNum")
sem.df<- sem.df[, c(1,5,9:18,42,45,48,49,51,53:56)]
prac.ef.mod <- ' 
        # measurement models :
          seas.migr =~ 1*p3s + p2s + pl
          otor =~ 1*p4 + p5 + p9
          resv.past =~ 1*p7 + p6 + p8 + p10
          ef1 =~ PerrGrass + LitterCover +RRC + SRD + Bare.inv + Shrubs
          ef2 =~ PerrForb +Sedges + SpRich + Shrubs
        # regressions
          ef1 ~ seas.migr + otor + resv.past
          ef2 ~ seas.migr + otor + resv.past
        # correlations
          ef1 ~~ ef2
          p2s ~~ 1*p2s
         # seas.migr~~otor
        #  seas.migr~~resv.past
         # resv.past ~~ otor
          '
ord2 <- c("p4" , "p5", "p6" ,"p7" , "p8", "p9", "p10")
sem.df.ez<- dplyr::filter(sem.df, ez != 3)%>%mutate(ez, funs(factor(.)))
fit.mod <- sem(prac.ef.mod,  data= sem.df, std.lv = TRUE, ordered = ord2)#, group = "CBRM")
summary(fit.mod, rsquare = T, standardized = T, fit.measures=T)
semPaths(fit.mod,  nCharNodes= 0, "std",
         whatLabels = "std", layout= "tree", 
         residuals = FALSE, intercepts = FALSE, edge.label.cex=1)

```



